<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>importance-of-text-pre-processing</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<p><span data-css-15b13by="" aria-hidden="false">Get started</span></p>
<p><span data-css-15b13by="" aria-hidden="false">Log in</span></p>
<p><img src="../../pluralsight.imgix.net/author/lg/c7859b4f-a0e9-4f74-8559-62f43bdcabea.jpeg" alt="Author avatar" class="jsx-3841407315" /></p>
<p>Gaurav Singhal</p>
<h1 id="importance-of-text-pre-processing">Importance of Text Pre-processing</h1>
<h3 id="gaurav-singhal">Gaurav Singhal</h3>
<ul>
<li><p>Oct 5, 2020</p></li>
<li><p>12 Min read</p></li>
<li><p>5,093 Views</p></li>
<li><p>Oct 5, 2020</p></li>
<li><p><span class="jsx-3759398792" itemprop="timeRequired">12 Min</span> read</p></li>
<li><p>5,093 Views</p></li>
</ul>
<p><span class="jsx-3759398792"></span></p>
<p><span data-css-1997kh1="">Data</span></p>
<p><span class="jsx-3759398792"></span></p>
<p><span data-css-1997kh1="">Data Analytics</span></p>
<p><span class="jsx-3759398792"></span></p>
<p><span data-css-1997kh1="">Machine Learning</span></p>
<p>Introduction</p>
<p>26</p>
<ul>
<li><a href="#module-introduction" class="menu-link">Introduction</a></li>
<li><a href="#module-gettingstarted" class="menu-link">Getting Started</a></li>
<li><a href="#module-cleaningandremovingnoise" class="menu-link">Cleaning and Removing Noise</a></li>
<li><a href="#module-nltk" class="menu-link">NLTK</a></li>
<li><a href="#module-conclusion" class="menu-link">Conclusion</a></li>
<li><a href="#top" class="menu-link">Top</a></li>
</ul>
<h2 id="introduction">Introduction</h2>
<p>If, as they say, “the customer is king,” then customer feedback is vital for any organization, and even to the government of any country. Feedback has the power to make or break a government or organization. The insights gained through public review analysis can influence strategy for better performance.</p>
<p>The kind of data you get from customer feedback is usually unstructured. It contains unusual text and symbols that need to be cleaned so that a machine learning model can grasp it. Data cleaning and pre-processing are as important as building any sophisticated machine learning model. The reliability of your model is highly dependent upon the quality of your data.</p>
<h2 id="getting-started">Getting Started</h2>
<p>Steps in pre-processing depend upon the given task and volume of data. This guide will cover main pre-processing techniques like leaning, normalization, tokenization, and annotation.</p>
<p>Before going further with these techniques, import important libraries.</p>
<pre><code>1import numpy as np
2import pandas as pd 
3import re
4import string
5import nltk
6from nltk.corpus import stopwords
7from nltk.tokenize import word_tokenize 
8from nltk.stem.porter import *
9from nltk.stem.wordnet import WordNetLemmatizer</code></pre>
<p>python</p>
<h2 id="cleaning-and-removing-noise">Cleaning and Removing Noise</h2>
<p>It helps to get rid of unhelpful parts of the data, or <em>noise</em>, by converting all characters to lowercase, removing punctuations marks, and removing stop words and typos.</p>
<p>Removing noise comes in handy when you want to do text analysis on pieces of data like comments or tweets. The code in the following sections will be helpful to get rid of the text that interferes with text analysis.</p>
<p>This example uses simple text for an easy walkthrough. You may also add <span class="jsx-3120878690"><code>.txt</code></span> files with paragraphs, or paste/write them directly. Refer to the alternative code below.</p>
<h3 id="lowercase">Lowercase</h3>
<p>You might be thinking, “What should be my approach when capitalization is at the beginning of the sentence or in proper nouns?” There is a common approach to lowercasing everything for the sake of simplicity. It helps to maintain the consistency flow during the NLP tasks and text mining. The <span class="jsx-3120878690"><code>lower()</code></span> function makes the whole process quite straightforward.</p>
<pre><code>1def lowercase(intext):
2    return intext.lower()    
3
4#Alternatively:
5#orig =data.raw(r&#39;file.text&#39;) # read raw text form orignal file
6#sent = data.sents(r&#39;file.txt&#39;) #brake parah into sentance
7#bwords = data.words(r&#39;file.text&#39;)#break parah into words
8
9intext = input(&#39;Your-Text:&#39;)
10clean_text = lowercase(intext)
11print(&#39;\nlowercased:&#39;,lowercase(clean_text))</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/9d3757c2-4803-43aa-8d9e-01791fc8d500_1.html" /><figcaption aria-hidden="true">lowercase removal</figcaption>
</figure>
<p>The punctuation to the sentence adds up <em>noise</em> that brings ambiguity while training the model.</p>
<h3 id="punctuations">Punctuations</h3>
<p>Let’s check the types of punctuation the <span class="jsx-3120878690"><code>string.punctuation()</code></span> function filters out. To achieve the punctuation removal, <span class="jsx-3120878690"><code>maketrans()</code></span> is used. It can replace the specific characters’ punctuation, in this case with some other character. The code replaces the punctuation with spaces (<span class="jsx-3120878690"><code>''</code></span>). <span class="jsx-3120878690"><code>translate()</code></span> is a function used to make these replacements.</p>
<pre><code>1output= string.punctuation
2print(&#39;list of punctuations:&#39;, output)
3def punctuation_cleaning(intext):
4    return text.translate(str.maketrans(&#39;&#39;, &#39;&#39;, output))
5print(&#39;\nNo-punctuation:&#39;,punctuation_cleaning(clean_text))</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/d93b8c53-1dd8-42f8-8b3f-2c4cf0766d52_2.html" /><figcaption aria-hidden="true">removing punctuations</figcaption>
</figure>
<h3 id="html-code-and-url-links">HTML Code and URL Links</h3>
<p>The code below uses regular expressions (<span class="jsx-3120878690"><code>re)</code></span>. To perform matches with a regular expression, use <span class="jsx-3120878690"><code>re.complie</code></span> to convert them into objects so that searching for patterns becomes easier and string substitution can be performed. A <span class="jsx-3120878690"><code>.sub()</code></span> function is used for this.</p>
<pre><code>1def url_remove(text):
2    url_pattern = re.compile(r&#39;https?://\S+|www\.\S+&#39;)
3    return url_pattern.sub(r&#39;&#39;, text)
4def html_remove(text):
5    html_pattern = re.compile(&#39;&lt;.*?&gt;&#39;)
6    return html_pattern.sub(r&#39;&#39;, text)
7
8text1 = input(&#39;Your-Text:&#39;)
9print(&#39;\nNo-url-links:&#39;, url_remove(text1))
10text2 = input(&#39;Your Text:&#39;)
11print(&#39;\nNo-html-codes:&#39;, html_remove(text2))</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/15d97c78-2d0f-4625-8cf0-b417fb885256_3.html" /><figcaption aria-hidden="true">removing urls</figcaption>
</figure>
<h3 id="spell-checks">Spell Checks</h3>
<p>This guide uses the <span class="jsx-3120878690"><code>pyspellchecker</code></span> package for spelling correction.</p>
<pre><code>1from spellchecker import SpellChecker
2
3spelling = SpellChecker()
4def spelling_checks(text):
5    correct_result = []
6    typo_words = spelling.unknown(text.split())
7    for word in text.split():
8        if word in typo_words:
9            correct_result.append(spelling.correction(word))
10        else:
11            correct_result.append(word)
12    return &quot; &quot;.join(correct_result)
13        
14text = input(&#39;Your-Text: &#39;)
15print(&#39;Error free text:&#39;,spelling_checks(text))</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/2076141f-3c0a-4b2c-a631-27c1f9b53c72_4.html" /><figcaption aria-hidden="true">error free text</figcaption>
</figure>
<p>Refer to <a href="https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b">this code</a> to learn how to remove emojis from your text.</p>
<h2 id="nltk">NLTK</h2>
<p>NLTK stands for <em>Natural Language Toolkit</em>. It is a powerful tool complete with different Python modules and libraries to carry out simple to complex natural language processing (NLP). These NLP libraries act as translators between machines (like Alexa, Siri, or Google Assistant) and humans so that the machines have the appropriate response. NLTK has a large, structured text known as a <em>corpus</em> thatt contains machine-readable text files in a directory produced for NLP tasks. WordNet is a famous corpus reader. Read more about corpus readers <a href="https://www.nltk.org/api/nltk.corpus.html">here</a>.</p>
<p>Before using the NLTK library, make sure it is downloaded on your system. Use <a href="https://www.nltk.org/install.html">these steps</a> to install NLTK.</p>
<h3 id="tokenization">Tokenization</h3>
<p><em>Tokenizing</em> is like splitting a whole sentence into words. You can consider a simple separator for this purpose. But a separator will fail to split the abbreviations separated by “.” or special characters, like U.A.R.T., for example. Challenges increase when more languages are included. How about dealing with compound words in languages such as German or French?</p>
<p>Most of these problems can be solved by using the <span class="jsx-3120878690"><code>nltk</code></span> library. The <span class="jsx-3120878690"><code>word_tokenize</code></span> module breaks the words into tokens and these words act as an input for the normalization and cleaning process. It can further be used to convert a string (text) into numeric data so that machine learning models can digest it.</p>
<h3 id="removing-stop-words">Removing Stop Words</h3>
<p>English is one of the most common languages, especially in the world of social media. For instance, “a,” “our,” “for,” “in,” etc. are in the set of most commonly used words. Removing these words helps the model to consider only key features. These words also don’t carry much information. By eliminating them, data scientists can focus on the important words.</p>
<p>Check out the list of the stop words <span class="jsx-3120878690"><code>nltk</code></span> provides.</p>
<pre><code>1stopwordslist = stopwords.words(&#39;english&#39;)
2print(stopwordslist)
3print(&#39;Total:&#39;,len(stopwordslist))</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/0de5382f-798b-4eca-95a1-d964aa717288_5.html" /><figcaption aria-hidden="true">stop words list</figcaption>
</figure>
<pre><code>1text = &quot;A smart kid ran towards the police station when he saw the thieves approaching.&quot;
2stop_words = set(stopwords.words(&#39;english&#39;)) 
3tokenwords = word_tokenize(text) 
4result = [w for w in tokenwords if not w in stop_words] 
5result = [] 
6for w in tokenwords: 
7    if w not in stop_words: 
8        result.append(w) 
9  
10print(&#39;Tokenized words: &#39;,tokenwords) 
11print(&#39;No-Stopwords: &#39;,result) </code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/8af3fca6-5b1e-4d6e-8ee2-b7495733a521_6.html" /><figcaption aria-hidden="true">tokenized words</figcaption>
</figure>
<p>The highlighted words are removed from the sequence. Put in some dummy text and notice the changes.</p>
<h3 id="normalization">Normalization</h3>
<p><em>Normalization</em> is an advanced step in cleaning to maintain uniformity. It brings all the words under on the roof by adding <em>stemming</em> and <em>lemmatization</em>. Many people often get stemming and lemmatizing confused. It’s true that they are both normalization processes, but they are a lot different. <embed src="../../pluralsight2.imgix.net/guides/c71d705d-445d-4d4c-99d1-38ce48985cba_12.html" /></p>
<h3 id="stemming">Stemming</h3>
<p>There are many variations of words that do not bring any new information and create redundancy, ultimately bringing ambiguity when training machine learning models for predictions. Take “He likes to walk” and “He likes walking,” for example. Both have the same meaning, so the <span class="jsx-3120878690"><code>stemming</code></span> function will remove the suffix and convert “walking” to “walk.” The example in this guide uses the <span class="jsx-3120878690"><code>PorterStemmer</code></span> module to conduct the process. You can use the <span class="jsx-3120878690"><code>snowball</code></span> module for different languages.</p>
<pre><code>1ps = PorterStemmer()
2stemwords = [ps.stem(w) for w in tokenwords]
3print (&#39;Stemming-Form:&#39;, stemwords)</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/9719d469-f1d3-44a7-aade-667bac19235a_7.html" /><figcaption aria-hidden="true">stemming forms</figcaption>
</figure>
<p>In this example, the words “polic” and “thiev” don’t make sense. They have their <span class="jsx-3120878690"><code>e</code></span> and <span class="jsx-3120878690"><code>es</code></span> clipped due to stemming’s suffix stripping rule. The Lemmatization technique can address this problem.</p>
<h3 id="lemmatization">Lemmatization</h3>
<p>Unlike stemming, <em>lemmatization</em> performs normalization using vocabulary and morphological analysis of words. Lemmatization aims to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the <em>lemma</em>. Lemmatization uses a dictionary, which makes it slower than stemming, however the results make much more sense than what you get from stemming. Lemmatization is built on WordNet’s built-in morphy function, making it an intelligent operation for text analysis. A <a href="https://wordnet.princeton.edu/">WordNet</a> module is a large and public lexical database for the English language. Its aim is to maintain the structured relationship between the words. The <span class="jsx-3120878690"><code>WordNetLemmitizer()</code></span> is the earliest and most widely used function.</p>
<pre><code>1lemmatizer = WordNetLemmatizer()
2lemmawords = [lemmatizer.lemmatize(w) for w in tokenwords]
3print (&#39;Lemmtization-form&#39;,lemmawords)</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/8bc686b5-0ebb-423d-9629-0d4129fab79f_8.html" /><figcaption aria-hidden="true">lemmatization form</figcaption>
</figure>
<h3 id="use-case">Use Case</h3>
<p>The NLTK corpus reader uses a lexical database to find a word’s synonyms, antonyms, hypernyms, etc. In this use case, you will find the <em>synonyms</em> (words that have the same meaning) and <em>hypernyms</em> (words that give a broader meaning) for a word by using the <span class="jsx-3120878690"><code>synset()</code></span>function.</p>
<pre><code>1from nltk.corpus import wordnet as wn
2
3for ssn in wn.synsets(&#39;aid&#39;):
4    print(&#39;\nName:&#39;,ssn.name(),&#39;\n-Abstract term: &#39;,ssn.hypernyms(),&#39;\n-Specific term:&#39;,ssn.hyponyms())#Try:ssn.root_hypernyms()</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/c1543285-b4b9-4819-b6e1-57ce718848c9_10.html" /><figcaption aria-hidden="true">ssn</figcaption>
</figure>
<p>There are three abstract terms for the word “aid”. The <span class="jsx-3120878690"><code>definition()</code></span> and <span class="jsx-3120878690"><code>examples()</code></span> functions in WordNet will help clarify the context.</p>
<pre><code>1print(&#39;Meaning:&#39; ,wn.synset(&#39;aid.n.01&#39;).definition()) #try any term-eg: care.n.01
2print(&#39;Example: &#39;,wn.synset(&#39;aid.n.01&#39;).examples())</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/7f070c9b-aa26-4393-b907-6ed35036c9e5_11.html" /><figcaption aria-hidden="true">Meaning and example</figcaption>
</figure>
<h3 id="part-of-speech-tagging-pos">Part of Speech Tagging (POS)</h3>
<p>In the English language, one word can have different grammatical contexts, and in these cases it’s not a good practice to consider the two words redundant. POS aims to make them grammatically unique.</p>
<pre><code>1text_words = word_tokenize(text)
2nltk.pos_tag(text_words)</code></pre>
<p>python</p>
<figure>
<embed src="../../pluralsight2.imgix.net/guides/c2cad28e-5f79-4222-a0af-dc224093911d_9.html" /><figcaption aria-hidden="true">POS</figcaption>
</figure>
<h2 id="conclusion">Conclusion</h2>
<p>These are key techniques that most data scientists follow before going further for analysis. Many of them have claimed that text pre-processing has degraded the performance of their machine learning model. Hence, combining these tools and techniques is a complex task.</p>
<p>It is not necessary to conduct all of the above techniques. You must understand the type of data you are dealing with and accordingly apply the ones that give the best results. Apply moderate pre-processing if you have a lot of noisy data, or if you have good quality text but a scarcity of data. When the data is sparse, heavy text pre-processing is needed.</p>
<p>Because the input text is customizable, you may try creating your sentences or inserting raw text a file and pre-process it. NLTK is a powerful tool. Machines are learning human languages. We are stepping into a whole new world!</p>
<p>Feel free to reach to me <a href="http://gauravsinghal.me/">here</a>.</p>
<p>26</p>
<p><a href="https://www.pluralsight.com/product/paths"><span data-css-15b13by="" aria-hidden="false">LEARN MORE</span></a></p>
</body>
</html>
